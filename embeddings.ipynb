{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89f1497a",
   "metadata": {},
   "source": [
    "# LLM Text Preprocessing Foundations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3813968",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "\n",
    "\n",
    "This notebook follows the ideas from Chapter 2 of *Build a Large Language Model (From Scratch)* by Sebastian Raschka.\n",
    "\n",
    "\n",
    "\n",
    "The goal is to understand how raw text is transformed into numerical representations that a neural network (an LLM or an agentic system) can use:\n",
    "\n",
    "\n",
    "\n",
    "- We load a real text dataset (`the-verdict.txt`).\n",
    "\n",
    "- We tokenize the text and map tokens to integer IDs.\n",
    "\n",
    "- We create sliding-window training samples using `max_length` and `stride`.\n",
    "\n",
    "- We build a small PyTorch dataset/dataloader.\n",
    "\n",
    "- We use an embedding layer to map token IDs into dense vectors.\n",
    "\n",
    "\n",
    "\n",
    "Throughout the notebook, additional markdown cells explain **why** each step (tokenization, windowing, embeddings) matters for training large language models and agentic systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99487e80",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a21fa9",
   "metadata": {},
   "source": [
    "## Environment & Dependencies\n",
    "\n",
    "This notebook was executed locally on macOS using Python 3.\n",
    "The required libraries are installed in the active environment:\n",
    "\n",
    "- torch\n",
    "- tiktoken\n",
    "- notebook / jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c29f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch tiktoken notebook ipykernel\n",
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "845c8fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bf6e395b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would ha\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"Characters:\", len(text))\n",
    "print(text[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db526aaf",
   "metadata": {},
   "source": [
    "## Tokenize Text\n",
    "\n",
    "Neural networks cannot understand raw text directly. They operate on numbers\n",
    "\n",
    "Tokenization converts text into smaller units (tokens), which are then mapped to numerical IDs, this process transforms human-readable language into machine-readable data.\n",
    "\n",
    "Without tokenization:\n",
    "- We cannot compute gradients\n",
    "- We cannot perform matrix multiplications\n",
    "- We cannot represent language mathematically\n",
    "\n",
    "Tokenization is the first essential step in building an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "42e6400f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 5145\n",
      "[40, 367, 2885, 1464, 1807, 3619, 402, 271, 10899, 2138, 257, 7026, 15632, 438, 2016, 257, 922, 5891, 1576, 438]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "tokens = tokenizer.encode(text)\n",
    "\n",
    "print(\"Number of tokens:\", len(tokens))\n",
    "print(tokens[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2831bf8",
   "metadata": {},
   "source": [
    "## What Are max_length and stride?\n",
    "\n",
    "- `max_length` defines the size of each training sequence (context window).\n",
    "- `stride` determines how much the window moves each step.\n",
    "\n",
    "This creates multiple overlapping training examples from a single long text.\n",
    "\n",
    "This is critical for autoregressive models:\n",
    "The model learns to predict the next token given previous tokens.\n",
    "\n",
    "Overlap helps the model:\n",
    "- Preserve contextual continuity\n",
    "- Learn smoother transitions\n",
    "- Use more training examples from limited data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f4745034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(token, max_length, stride):\n",
    "    input_ids = []\n",
    "    target_ids = []\n",
    "    \n",
    "    for i in range(0, len(token) - max_length, stride):\n",
    "        input_chunk = token[i:i+max_length]\n",
    "        target_chunk = token[i+1:i+max_length+1]\n",
    "        \n",
    "        input_ids.append(input_chunk)\n",
    "        target_ids.append(target_chunk)\n",
    "    \n",
    "    return torch.tensor(input_ids), torch.tensor(target_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d34cee67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 40\n",
      "Shape of one sample: torch.Size([40, 128])\n"
     ]
    }
   ],
   "source": [
    "max_length = 128\n",
    "stride = 128\n",
    "\n",
    "input_ids, target_ids = create_dataset(tokens, max_length, stride)\n",
    "\n",
    "print(\"Number of samples:\", input_ids.shape[0])\n",
    "print(\"Shape of one sample:\", input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "27899e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch input shape: torch.Size([4, 128])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "dataset = TensorDataset(input_ids, target_ids)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "batch = next(iter(dataloader))\n",
    "print(\"Batch input shape:\", batch[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebbedc5",
   "metadata": {},
   "source": [
    "## What Is an Embedding Layer?\n",
    "\n",
    "An embedding layer is a trainable lookup table.\n",
    "\n",
    "It maps discrete token IDs into dense continuous vectors.\n",
    "\n",
    "Instead of representing a word as a single number (e.g., 5023),\n",
    "we represent it as a vector like:\n",
    "\n",
    "[0.12, -0.45, 0.89, ..., 0.03]\n",
    "\n",
    "This allows the neural network to:\n",
    "- Capture semantic similarity\n",
    "- Learn distributed representations\n",
    "- Encode meaning geometrically in vector space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d9f893f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding output shape: torch.Size([40, 128, 256])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = tokenizer.n_vocab\n",
    "embedding_dim = 256\n",
    "\n",
    "embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "embedded_tokens = embedding(input_ids)\n",
    "\n",
    "print(\"Embedding output shape:\", embedded_tokens.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690ecf8a",
   "metadata": {},
   "source": [
    "## Why Do Embeddings Encode Meaning?\n",
    "\n",
    "Embeddings encode meaning because they are learned through gradient descent.\n",
    "\n",
    "During training, the model adjusts embedding vectors to minimize prediction error. \n",
    "If two words appear in similar contexts, their vectors are adjusted in similar directions.\n",
    "\n",
    "This leads to:\n",
    "\n",
    "- Words with similar meanings having similar vectors\n",
    "- Semantic relationships forming geometric patterns\n",
    "- Meaning emerging from statistical patterns\n",
    "\n",
    "Relation to Neural Networks:\n",
    "\n",
    "- Embeddings are parameters of the model\n",
    "- They are optimized using backpropagation\n",
    "- They live in a continuous vector space (latent space)\n",
    "- They are equivalent to a linear projection from discrete IDs into a dense space\n",
    "\n",
    "In neural network terms:\n",
    "An embedding layer is simply a matrix multiplication between a one-hot vector and a weight matrix.\n",
    "\n",
    "Therefore, embeddings are learned representations that transform symbolic language into numerical structures that neural networks can process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8c1130e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stride 128 → Samples: 40\n",
      "Stride 64 → Samples: 79\n",
      "max_length 64 & stride 32 → Samples: 159\n"
     ]
    }
   ],
   "source": [
    "# Experiment 1\n",
    "max_length = 128\n",
    "stride = 128\n",
    "\n",
    "input_ids_1, _ = create_dataset(tokens, max_length, stride)\n",
    "print(\"Stride 128 → Samples:\", input_ids_1.shape[0])\n",
    "\n",
    "# Experiment 2\n",
    "max_length = 128\n",
    "stride = 64\n",
    "\n",
    "input_ids_2, _ = create_dataset(tokens, max_length, stride)\n",
    "print(\"Stride 64 → Samples:\", input_ids_2.shape[0])\n",
    "\n",
    "# Experiment 3\n",
    "max_length = 64\n",
    "stride = 32\n",
    "\n",
    "input_ids_3, _ = create_dataset(tokens, max_length, stride)\n",
    "print(\"max_length 64 & stride 32 → Samples:\", input_ids_3.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af99d2d",
   "metadata": {},
   "source": [
    "## Experiment Analysis\n",
    "\n",
    "When stride is smaller:\n",
    "\n",
    "- The window overlaps more\n",
    "- More training samples are generated\n",
    "- The model sees similar context with slight shifts\n",
    "\n",
    "Why is overlap useful?\n",
    "\n",
    "Because language is sequential.\n",
    "If we split text without overlap, the model loses continuity between segments.\n",
    "\n",
    "Overlap allows the model to:\n",
    "- Learn smoother transitions\n",
    "- Capture dependencies across boundaries\n",
    "- Improve autoregressive prediction\n",
    "\n",
    "Reducing max_length:\n",
    "- Creates shorter contexts\n",
    "- Increases number of samples\n",
    "- But reduces long-range dependency learning\n",
    "\n",
    "This shows how preprocessing directly affects model learning capacity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5804f68",
   "metadata": {},
   "source": [
    "## Conclusion: from this pipeline to complete LLMs\n",
    "\n",
    "\n",
    "In this notebook, we saw the typical path that text data follows before entering a large model:\n",
    "\n",
    "\n",
    "1. **Raw text → tokens → IDs**: tokenization converts human language into sequences of integers on which matrix products can be performed and gradients calculated.\n",
    "2. **IDs → context windows**: with `max_length` and `stride`, we construct training examples that respect the order of the sequence and allow the model to learn to predict the next token.\n",
    "3. **IDs → dense embeddings**: the `Embedding` layer learns, via backpropagation, a vector space where the geometry reflects meaning and context.\n",
    "\n",
    "A complete LLM (e.g., a Transformer) takes exactly these embeddings as input, adds positional information to them, and then applies attention layers and linear layers to model complex long-term dependencies.\n",
    "\n",
    "In agentic systems, this same representation is the starting point for reasoning, planning, and decision-making: everything the agent “knows” about the text is compressed into these embeddings and refined through the layers of the model.\n",
    "\n",
    "Translated with DeepL.com (free version)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
